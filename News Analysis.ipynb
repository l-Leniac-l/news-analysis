{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brazilian Newspaper analysis\n",
    "\n",
    "In this project, we'll use a dataset from a Brazilian Newspaper called \"Folha de São Paulo\".\n",
    "\n",
    "We're going to use word embeddings, tensorboard and rnn's and search for political opinions and positions.\n",
    "\n",
    "You can find the dataset at [kaggle](https://www.kaggle.com/marlesson/news-of-the-site-folhauol).\n",
    "\n",
    "I want to find in this study case:\n",
    "\n",
    "+ Political opinions\n",
    "+ Check if this newspaper is impartial or biased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram model\n",
    "\n",
    "Let's use a word embedding model to find the relationship between words in the articles. Our model will learn how one word is related to another word and we'll see this relationship in tensorboard and a T-SNE chart (to project our model in a 2D chart).\n",
    "\n",
    "We have two options to use: CBOW (Continuous Bag-Of-Words) and Skip-gram.\n",
    "\n",
    "In our case we'll use Skip-gram because it performs better than CBOW.\n",
    "\n",
    "The models works like this:\n",
    "\n",
    "![](assets/word2vec_architectures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CBOW we get some words around another word and try to predict the \"middle\" word.\n",
    "\n",
    "In Skip-gram we do the opposite, we get one word and try to predict the words around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "After downloading the dataset, put it on a directory `data/` and let's load it using pandas.\n",
    "\n",
    "**Using python 3.6 and tensorflow 1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lula diz que está 'lascado', mas que ainda tem...</td>\n",
       "      <td>Com a possibilidade de uma condenação impedir ...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>poder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/poder/2017/10/192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Decidi ser escrava das mulheres que sofrem', ...</td>\n",
       "      <td>Para Oumou Sangaré, cantora e ativista malines...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>ilustrada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/ilustrada/2017/10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Três reportagens da Folha ganham Prêmio Petrob...</td>\n",
       "      <td>Três reportagens da Folha foram vencedoras do ...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>poder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/poder/2017/10/192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Filme 'Star Wars: Os Últimos Jedi' ganha trail...</td>\n",
       "      <td>A Disney divulgou na noite desta segunda-feira...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>ilustrada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/ilustrada/2017/10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBSS inicia acordos com fintechs e quer 30% do...</td>\n",
       "      <td>O CBSS, banco da holding Elopar dos sócios Bra...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>mercado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mercado/2017/10/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Lula diz que está 'lascado', mas que ainda tem...   \n",
       "1  'Decidi ser escrava das mulheres que sofrem', ...   \n",
       "2  Três reportagens da Folha ganham Prêmio Petrob...   \n",
       "3  Filme 'Star Wars: Os Últimos Jedi' ganha trail...   \n",
       "4  CBSS inicia acordos com fintechs e quer 30% do...   \n",
       "\n",
       "                                                text        date   category  \\\n",
       "0  Com a possibilidade de uma condenação impedir ...  2017-09-10      poder   \n",
       "1  Para Oumou Sangaré, cantora e ativista malines...  2017-09-10  ilustrada   \n",
       "2  Três reportagens da Folha foram vencedoras do ...  2017-09-10      poder   \n",
       "3  A Disney divulgou na noite desta segunda-feira...  2017-09-10  ilustrada   \n",
       "4  O CBSS, banco da holding Elopar dos sócios Bra...  2017-09-10    mercado   \n",
       "\n",
       "  subcategory                                               link  \n",
       "0         NaN  http://www1.folha.uol.com.br/poder/2017/10/192...  \n",
       "1         NaN  http://www1.folha.uol.com.br/ilustrada/2017/10...  \n",
       "2         NaN  http://www1.folha.uol.com.br/poder/2017/10/192...  \n",
       "3         NaN  http://www1.folha.uol.com.br/ilustrada/2017/10...  \n",
       "4         NaN  http://www1.folha.uol.com.br/mercado/2017/10/1...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/articles.csv')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unnecessary articles\n",
    "\n",
    "We are trying to find political opinions. So, let's take only the articles in category 'poder' (power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lula diz que está 'lascado', mas que ainda tem...</td>\n",
       "      <td>Com a possibilidade de uma condenação impedir ...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>poder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/poder/2017/10/192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Três reportagens da Folha ganham Prêmio Petrob...</td>\n",
       "      <td>Três reportagens da Folha foram vencedoras do ...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>poder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/poder/2017/10/192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Posso sair do Brasil quando e como quiser, diz...</td>\n",
       "      <td>O italiano Cesare Battisti disse nesta segunda...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>poder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/poder/2017/10/192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Supremo nega pedido para Senado analisar impea...</td>\n",
       "      <td>O STF (Supremo Tribunal Federal) negou na quin...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>poder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/poder/2017/10/192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dodge defende manter Joesley e Saud, da JBS, p...</td>\n",
       "      <td>A procuradora-geral da República, Raquel Dodge...</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>poder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/poder/2017/10/192...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Lula diz que está 'lascado', mas que ainda tem...   \n",
       "2   Três reportagens da Folha ganham Prêmio Petrob...   \n",
       "6   Posso sair do Brasil quando e como quiser, diz...   \n",
       "8   Supremo nega pedido para Senado analisar impea...   \n",
       "10  Dodge defende manter Joesley e Saud, da JBS, p...   \n",
       "\n",
       "                                                 text        date category  \\\n",
       "0   Com a possibilidade de uma condenação impedir ...  2017-09-10    poder   \n",
       "2   Três reportagens da Folha foram vencedoras do ...  2017-09-10    poder   \n",
       "6   O italiano Cesare Battisti disse nesta segunda...  2017-09-10    poder   \n",
       "8   O STF (Supremo Tribunal Federal) negou na quin...  2017-09-10    poder   \n",
       "10  A procuradora-geral da República, Raquel Dodge...  2017-09-10    poder   \n",
       "\n",
       "   subcategory                                               link  \n",
       "0          NaN  http://www1.folha.uol.com.br/poder/2017/10/192...  \n",
       "2          NaN  http://www1.folha.uol.com.br/poder/2017/10/192...  \n",
       "6          NaN  http://www1.folha.uol.com.br/poder/2017/10/192...  \n",
       "8          NaN  http://www1.folha.uol.com.br/poder/2017/10/192...  \n",
       "10         NaN  http://www1.folha.uol.com.br/poder/2017/10/192...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_dataset = dataset.loc[dataset.category == 'poder']\n",
    "\n",
    "political_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging title and text\n",
    "To maintain article titles and text related, let's merge then together and use this merged text as our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lula diz que está 'lascado', mas que ainda tem força como cabo eleitoral ---- Com a possibilidade de uma condenação impedir sua candidatura em 2018, o ex-presidente Luiz Inácio Lula da Silva fez, nesta segunda (9), um discurso inflamado contra a Lava Jato, no qual disse saber que está \"lascado\", exigiu um pedido de desculpas do juiz Sergio Moro e afirmou que, mesmo fora da disputa pelo Planalto, será um cabo eleitoral expressivo para a sucessão de Michel Temer.  Segundo o petista, réu em sete ações penais, o objetivo de Moro é impedir sua candidatura no ano que vem, desidratando-o, inclusive, no apoio a um nome alternativo, como o do ex-prefeito de São Paulo Fernando Haddad (PT), caso ele não possa concorrer à Presidência.  \"Eu sei que tô lascado, todo dia tem um processo. Eu não quero nem que Moro me absolva, eu só quero que ele peça desculpas\", disse Lula durante um seminário sobre educação em Brasília. \"Eles [investigadores] chegam a dizer: 'Ah, se o Lula não for candidato, ele não vai ter força como cabo eleitoral'. Testem\", completou o petista.  Para o ex-presidente, Moro usou \"mentiras contadas pela Polícia Federal e pelo Ministério Público\" para julgá-lo e condená-lo a nove anos e seis meses de prisão pelo caso do tríplex em Guarujá (SP).  O ex-presidente disse ainda não ter \"medo\" dos investigadores que, de acordo com ele, estão acostumados a \"mexer com deputados e senadores\" que temem as apurações.  \"Eu quero que eles saibam o seguinte: se eles estão acostumados a lidar com deputado que tem medo deles, a mexer com senadores que têm medo deles, quero dizer que tenho respeito profundo por quem me respeita, pelas leis que nós ajudamos a criar, mas não tenho respeito por quem não me respeita e eles não me respeitaram\", afirmou o petista.    De acordo com aliados, Lula não gosta de discutir, mesmo que nos bastidores, a chance de não ser candidato ao Planalto e a projeção do nome de Haddad como plano B do PT tem incomodado os mais próximos ao ex-presidente. O ex-prefeito, que estava no evento nesta segunda, fez um discurso rápido, de menos de dez minutos, em que encerrou dizendo esperar que Lula assuma a Presidência em 2019.  \"Espero que dia 1º de janeiro de 2019 esse pesadelo chamado Temer acabe e o senhor assuma a Presidência da República\", disse Haddad.  'DEMÔNIO DO MERCADO'  Lula voltou a fazer um discurso mais agressivo em relação ao mercado e disse que \"não tem cara de demônio\", mas quer que o respeitem \"como se fosse\".  \"Não tenho cara de demônio, mas quero que eles me respeitem como se eu fosse, porque eles sabem que a economia não vai ficar subordinada ao elitismo da sociedade brasileira\", disse o ex-presidente.  O petista rivalizou ainda com o deputado Jair Bolsonaro (PSC-RJ), segundo colocado nas últimas pesquisas empatado com Marina Silva, e disse que se ele \"agrada ao mercado\", o PT tem que \"desagradar\".  A Folha publicou nesta segunda (9) reportagem em que mostrou que o deputado ensaia movimento ao centro no debate econômico, adotando um discurso simpático aos investidores do mercado financeiro.\n"
     ]
    }
   ],
   "source": [
    "# Merges the title and text with a separator (---)\n",
    "merged_text = [str(title) + ' ---- ' + str(text) for title, text in zip(political_dataset.title, political_dataset.text)]\n",
    "\n",
    "print(merged_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing punctuation\n",
    "We need to tokenize all text punctuation, otherwise the network will see punctuated words differently (eg: hello != hello!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lula diz que está  ||single-quote|| lascado ||single-quote||  ||comma||  mas que ainda tem força como cabo eleitoral  ||dash||  ||dash||  Com a possibilidade de uma condenação impedir sua candidatura em 2018 ||comma||  o ex-presidente Luiz Inácio Lula da Silva fez ||comma||  nesta segunda  ||parentheses-left|| 9 ||parentheses-right||  ||comma||  um discurso inflamado contra a Lava Jato ||comma||  no qual disse saber que está  ||quote|| lascado ||quote||  ||comma||  exigiu um pedido de desculpas do juiz Sergio Moro e afirmou que ||comma||  mesmo fora da disputa pelo Planalto ||comma||  será um cabo eleitoral expressivo para a sucessão de Michel Temer ||period||   Segundo o petista ||comma||  réu em sete ações penais ||comma||  o objetivo de Moro é impedir sua candidatura no ano que vem ||comma||  desidratando-o ||comma||  inclusive ||comma||  no apoio a um nome alternativo ||comma||  como o do ex-prefeito de São Paulo Fernando Haddad  ||parentheses-left|| PT ||parentheses-right||  ||comma||  caso ele não possa concorrer à Presidência ||period||    ||quote|| Eu sei que tô lascado ||comma||  todo dia tem um processo ||period||  Eu não quero nem que Moro me absolva ||comma||  eu só quero que ele peça desculpas ||quote||  ||comma||  disse Lula durante um seminário sobre educação em Brasília ||period||   ||quote|| Eles  ||brackets-left|| investigadores ||brackets-right||  chegam a dizer ||colon||   ||single-quote|| Ah ||comma||  se o Lula não for candidato ||comma||  ele não vai ter força como cabo eleitoral ||single-quote||  ||period||  Testem ||quote||  ||comma||  completou o petista ||period||   Para o ex-presidente ||comma||  Moro usou  ||quote|| mentiras contadas pela Polícia Federal e pelo Ministério Público ||quote||  para julgá-lo e condená-lo a nove anos e seis meses de prisão pelo caso do tríplex em Guarujá  ||parentheses-left|| SP ||parentheses-right||  ||period||   O ex-presidente disse ainda não ter  ||quote|| medo ||quote||  dos investigadores que ||comma||  de acordo com ele ||comma||  estão acostumados a  ||quote|| mexer com deputados e senadores ||quote||  que temem as apurações ||period||    ||quote|| Eu quero que eles saibam o seguinte ||colon||  se eles estão acostumados a lidar com deputado que tem medo deles ||comma||  a mexer com senadores que têm medo deles ||comma||  quero dizer que tenho respeito profundo por quem me respeita ||comma||  pelas leis que nós ajudamos a criar ||comma||  mas não tenho respeito por quem não me respeita e eles não me respeitaram ||quote||  ||comma||  afirmou o petista ||period||     De acordo com aliados ||comma||  Lula não gosta de discutir ||comma||  mesmo que nos bastidores ||comma||  a chance de não ser candidato ao Planalto e a projeção do nome de Haddad como plano B do PT tem incomodado os mais próximos ao ex-presidente ||period||  O ex-prefeito ||comma||  que estava no evento nesta segunda ||comma||  fez um discurso rápido ||comma||  de menos de dez minutos ||comma||  em que encerrou dizendo esperar que Lula assuma a Presidência em 2019 ||period||    ||quote|| Espero que dia 1º de janeiro de 2019 esse pesadelo chamado Temer acabe e o senhor assuma a Presidência da República ||quote||  ||comma||  disse Haddad ||period||    ||single-quote|| DEMÔNIO DO MERCADO ||single-quote||   Lula voltou a fazer um discurso mais agressivo em relação ao mercado e disse que  ||quote|| não tem cara de demônio ||quote||  ||comma||  mas quer que o respeitem  ||quote|| como se fosse ||quote||  ||period||    ||quote|| Não tenho cara de demônio ||comma||  mas quero que eles me respeitem como se eu fosse ||comma||  porque eles sabem que a economia não vai ficar subordinada ao elitismo da sociedade brasileira ||quote||  ||comma||  disse o ex-presidente ||period||   O petista rivalizou ainda com o deputado Jair Bolsonaro  ||parentheses-left|| PSC-RJ ||parentheses-right||  ||comma||  segundo colocado nas últimas pesquisas empatado com Marina Silva ||comma||  e disse que se ele  ||quote|| agrada ao mercado ||quote||  ||comma||  o PT tem que  ||quote|| desagradar ||quote||  ||period||   A Folha publicou nesta segunda  ||parentheses-left|| 9 ||parentheses-right||  reportagem em que mostrou que o deputado ensaia movimento ao centro no debate econômico ||comma||  adotando um discurso simpático aos investidores do mercado financeiro ||period|| \n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    tokens = {\n",
    "        '.'  : 'period',\n",
    "        ','  : 'comma',\n",
    "        '\"'  : 'quote',\n",
    "        '\\'' : 'single-quote',\n",
    "        ';'  : 'semicolon',\n",
    "        ':'  : 'colon',\n",
    "        '!'  : 'exclamation-mark',\n",
    "        '?'  : 'question-mark',\n",
    "        '('  : 'parentheses-left',\n",
    "        ')'  : 'parentheses-right',\n",
    "        '['  : 'brackets-left',\n",
    "        ']'  : 'brackets-right',\n",
    "        '{'  : 'braces-left',\n",
    "        '}'  : 'braces-right',\n",
    "        '_'  : 'underscore',\n",
    "        '--' : 'dash',\n",
    "        '\\n' : 'return'\n",
    "    }\n",
    "    \n",
    "    return {token: '||{0}||'.format(value) for token, value in tokens.items()}\n",
    "\n",
    "token_dict = token_lookup()\n",
    "\n",
    "tokenized_text = []\n",
    "\n",
    "for text in merged_text:\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "    \n",
    "    tokenized_text.append(text)\n",
    "\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup tables\n",
    "\n",
    "We need to create two dicts: `word_to_int` and `int_to_word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten vocab words: \n",
      "[('a-', 0), ('retrocederem', 1), ('airton', 2), ('mab', 3), ('lovanni', 4), ('retratem', 5), ('filippeli', 6), ('roousseff', 7), ('monarquismo', 8), ('pintaram', 9)]\n",
      "\n",
      "Vocab length:\n",
      "97648\n"
     ]
    }
   ],
   "source": [
    "def lookup_tables(tokenized_text):\n",
    "    vocab = set()\n",
    "    \n",
    "    for text in tokenized_text:\n",
    "        text = text.lower()\n",
    "        vocab = vocab.union(set(text.split()))\n",
    "    \n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(vocab)}\n",
    "    \n",
    "    return vocab, vocab_to_int, int_to_vocab\n",
    "\n",
    "vocab, vocab_to_int, int_to_vocab = lookup_tables(tokenized_text)\n",
    "\n",
    "print('First ten vocab words: ')\n",
    "print(list(vocab_to_int.items())[0:10])\n",
    "print('\\nVocab length:')\n",
    "print(len(vocab_to_int))\n",
    "\n",
    "pickle.dump((tokenized_text, vocab, vocab_to_int, int_to_vocab, token_dict), open('preprocess/preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all text to integers\n",
    "\n",
    "Let's convert all articles to integer using the `vocab_to_int` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_text, vocab, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess/preprocess.p',  mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_int(text):\n",
    "    int_text = []\n",
    "    for word in text.split():\n",
    "        if word in vocab_to_int.keys():\n",
    "            int_text.append(vocab_to_int[word])\n",
    "    return np.asarray(int_text, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_articles_to_int(tokenized_text):\n",
    "    all_int_text = []\n",
    "    for text in tokenized_text:\n",
    "        all_int_text.append(text_to_int(text))\n",
    "    return np.asarray(all_int_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "converted_text = convert_articles_to_int(tokenized_text)\n",
    "\n",
    "pickle.dump((converted_text, vocab, vocab_to_int, int_to_vocab, token_dict), open('preprocess/preprocess2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "converted_text, vocab, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess/preprocess2.p',  mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63492, 73324, 76200, 71471, 96956, 84961, 65583, 65583, 87770,\n",
       "       49229, 88177, 82219, 11830, 87770, 39452, 49229, 46497, 65533,\n",
       "       76200, 71471, 46497, 73324, 84961, 96956, 43111, 53034, 59515,\n",
       "       56189, 68360, 91183, 66314, 49102, 27637, 82941, 49723, 46497,\n",
       "       82870, 84961, 90879, 21712, 78432, 24666, 69666, 46497, 52651,\n",
       "       84961, 93551, 12244, 57464,  2798, 22159,  9224,  2798, 21712,\n",
       "       87770, 49229, 21712, 78432, 21712, 27637, 37398, 84961, 15766,\n",
       "       21712, 82472, 46497, 73324, 84961, 96956, 43111, 56189, 84612,\n",
       "       90879, 21712,  4699, 55237, 69905,  2277, 66443, 53034, 73324,\n",
       "       84961, 96956, 11359, 49229, 49723,  4699, 38418, 30357, 56281,\n",
       "       76200, 66672, 21712, 28503, 78432, 95264, 30357, 73982,   378,\n",
       "        2798, 13306, 49229, 49723, 20010, 92464, 12641, 87770, 49723,\n",
       "       80507, 49229, 84961, 45411, 53034, 88132, 68680, 69905, 56189,\n",
       "       17248, 21712, 53034, 88132, 93894,  9661, 95022, 90655, 31291,\n",
       "       56189, 90189, 73982, 55070, 49723, 84961, 21712, 21712,   378,\n",
       "       21712, 21712, 90655, 56189,  2798, 68360, 21712, 88177, 53034,\n",
       "       73324,  2798, 82870, 21712, 78432, 54201, 56189, 21712, 69905,\n",
       "       12692, 84612, 40852, 76001, 34645, 59750, 44775, 78432, 35074,\n",
       "       53034, 22159,  9224,  2798, 21712,  4699, 95507, 44775, 85659,\n",
       "       72413, 88974, 12045, 94787, 21712,   228, 20196, 44775,  8515,\n",
       "       70152,  8555, 90655, 70152, 14713, 49723, 94506, 50730, 21712,\n",
       "       54264,  1494, 21391, 84961, 47590, 76826, 89837, 69905, 56189,\n",
       "       68360, 91183, 71886, 54264, 34284, 21670,  2798,  5418, 84612,\n",
       "       54423, 10646, 84961, 37398, 90655, 39452, 84961, 75950, 56189,\n",
       "        4699,  1888, 90655, 39270, 53034, 59515, 21712, 78432, 67473,\n",
       "       55901, 57350, 56189, 75883, 37639, 95067, 43111, 53034, 89936,\n",
       "         378, 47100, 56189], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling text\n",
    "\n",
    "We need to subsample our text and remove the words that not provides meaningful information, like: 'the', 'of', 'for'.\n",
    "\n",
    "Let's use Mikolov's subsampling formula, that's give us the probability of a word to be discarted:\n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "Where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts all articles to one big text\n",
    "\n",
    "all_converted_text = np.concatenate(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght before sumsampling: 10536901\n",
      "Lenght after sumsampling: 2134935\n"
     ]
    }
   ],
   "source": [
    "def subsampling(int_words, threshold=1e-5):\n",
    "    word_counts = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "    \n",
    "    return np.asarray(train_words)\n",
    "\n",
    "subsampled_text = subsampling(all_converted_text)\n",
    "\n",
    "print('Lenght before sumsampling: {0}'.format(len(all_converted_text)))\n",
    "print('Lenght after sumsampling: {0}'.format(len(subsampled_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((subsampled_text, vocab, vocab_to_int, int_to_vocab, token_dict), open('preprocess/preprocess3.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsampled_text, vocab, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess/preprocess3.p',  mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save vocab to csv\n",
    "\n",
    "Let's save our vocab to csv file, so that way we can use it as an embedding on tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsampled_ints = set(subsampled_text)\n",
    "\n",
    "subsampled_vocab = []\n",
    "\n",
    "for word in subsampled_ints:\n",
    "    subsampled_vocab.append(int_to_vocab[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>retrocederem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>airton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lovanni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0            a-\n",
       "1  retrocederem\n",
       "2        airton\n",
       "3           mab\n",
       "4       lovanni"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df = pd.DataFrame.from_dict(int_to_vocab, orient='index')\n",
    "\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_df.to_csv('preprocess/vocab.tsv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generate batches\n",
    "\n",
    "Now, we need to convert all text to numbers with lookup tables and create a batch generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    words = words.flat\n",
    "    words = list(words)\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Embedding Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_placeholders(graph, reuse=False):\n",
    "    with graph.as_default():\n",
    "        with tf.variable_scope('placeholder', reuse=reuse):\n",
    "            inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "            labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "            learning_rate = tf.placeholder(tf.float32, [None], name='learning_rate')\n",
    "            \n",
    "            return inputs, labels, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed_embeddings(graph, vocab_size, embedding_size, inputs, reuse=False):\n",
    "    with graph.as_default():\n",
    "        with tf.variable_scope('embedding', reuse=reuse):\n",
    "            embedding = tf.Variable(tf.random_uniform((vocab_size, embedding_size),\n",
    "                                                      -0.5 / embedding_size,\n",
    "                                                      0.5 / embedding_size))\n",
    "            embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "            \n",
    "            return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nce_weights_biases(graph, vocab_size, embedding_size, reuse=False):\n",
    "    with graph.as_default():\n",
    "        with tf.variable_scope('nce', reuse=reuse):\n",
    "            nce_weights = tf.Variable(tf.truncated_normal((vocab_size, embedding_size),\n",
    "                                                           stddev=1.0/math.sqrt(embedding_size)))\n",
    "            nce_biases = tf.Variable(tf.zeros(vocab_size))\n",
    "            \n",
    "            # Historigram for tensorboard\n",
    "            tf.summary.histogram('weights', nce_weights)\n",
    "            tf.summary.histogram('biases', nce_biases)\n",
    "\n",
    "            return nce_weights, nce_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_loss(graph, num_sampled, nce_weights, nce_biases, labels, embed, vocab_size, reuse=False):\n",
    "    with graph.as_default():\n",
    "        with tf.variable_scope('nce', reuse=reuse):\n",
    "            loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=nce_weights,\n",
    "                                                             biases=nce_biases,\n",
    "                                                             labels=labels,\n",
    "                                                             inputs=embed,\n",
    "                                                             num_sampled=num_sampled,\n",
    "                                                             num_classes=vocab_size))\n",
    "            \n",
    "            # Scalar for tensorboard\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed_opt(graph, learning_rate, loss, reuse=False):\n",
    "    with graph.as_default():\n",
    "        with tf.variable_scope('optmizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "            \n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_embed(graph,\n",
    "                batch_size,\n",
    "                learning_rate,\n",
    "                epochs,\n",
    "                window_size,\n",
    "                train_words,\n",
    "                num_sampled,\n",
    "                embedding_size,\n",
    "                vocab_size,\n",
    "                save_dir,\n",
    "                print_every):\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "\n",
    "        inputs, labels, lr = get_embed_placeholders(graph)\n",
    "\n",
    "        embed = get_embed_embeddings(graph, vocab_size, embedding_size, inputs)\n",
    "\n",
    "        nce_weights, nce_biases = get_nce_weights_biases(graph, vocab_size, embedding_size, reuse=True)\n",
    "\n",
    "        loss = get_embed_loss(graph, num_sampled, nce_weights, nce_biases, labels, embed, vocab_size, reuse=True)\n",
    "\n",
    "        optimizer = get_embed_opt(graph, learning_rate, loss, reuse=True)\n",
    "        \n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(save_dir)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        avg_loss = 0\n",
    "        iteration = 1\n",
    "\n",
    "        for e in range(1, epochs + 1):\n",
    "            batches = get_batches(train_words, batch_size, window_size)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            for x, y in batches:\n",
    "                feed = {\n",
    "                    inputs: x,\n",
    "                    labels: np.array(y)[:, None]\n",
    "                }\n",
    "\n",
    "                summary, _, train_loss  = sess.run([merged_summary, optimizer, loss], feed_dict=feed)\n",
    "\n",
    "                avg_loss += train_loss\n",
    "                \n",
    "                train_writer.add_summary(summary, epochs + 1)\n",
    "\n",
    "                if iteration % print_every == 0: \n",
    "                    end = time.time()\n",
    "                    print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                          \"Batch: {}\".format(iteration),\n",
    "                          \"Training loss: {:.4f}\".format(avg_loss/print_every),\n",
    "                          \"Speed: {:.4f} sec/batch\".format((end-start)/print_every))\n",
    "                    avg_loss = 0\n",
    "                    start = time.time()\n",
    "                #break\n",
    "                iteration += 1\n",
    "                \n",
    "        save_path = saver.save(sess, save_dir + '/embed.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Batch: 1000 Training loss: 3.2113 Speed: 0.6224 sec/batch\n",
      "Epoch 1/10 Batch: 2000 Training loss: 4.0933 Speed: 0.6024 sec/batch\n",
      "Epoch 2/10 Batch: 3000 Training loss: 4.3585 Speed: 0.5493 sec/batch\n",
      "Epoch 2/10 Batch: 4000 Training loss: 4.4642 Speed: 0.6041 sec/batch\n",
      "Epoch 3/10 Batch: 5000 Training loss: 4.4193 Speed: 0.5077 sec/batch\n",
      "Epoch 3/10 Batch: 6000 Training loss: 4.2437 Speed: 0.6098 sec/batch\n",
      "Epoch 4/10 Batch: 7000 Training loss: 4.4818 Speed: 0.4570 sec/batch\n",
      "Epoch 4/10 Batch: 8000 Training loss: 4.2949 Speed: 0.6150 sec/batch\n",
      "Epoch 5/10 Batch: 9000 Training loss: 4.7017 Speed: 0.4088 sec/batch\n",
      "Epoch 5/10 Batch: 10000 Training loss: 4.5748 Speed: 0.6180 sec/batch\n",
      "Epoch 6/10 Batch: 11000 Training loss: 4.8532 Speed: 0.3605 sec/batch\n",
      "Epoch 6/10 Batch: 12000 Training loss: 5.0591 Speed: 0.6264 sec/batch\n",
      "Epoch 7/10 Batch: 13000 Training loss: 4.8968 Speed: 0.3107 sec/batch\n",
      "Epoch 7/10 Batch: 14000 Training loss: 5.1024 Speed: 0.6286 sec/batch\n",
      "Epoch 8/10 Batch: 15000 Training loss: 5.1757 Speed: 0.2598 sec/batch\n",
      "Epoch 8/10 Batch: 16000 Training loss: 5.1417 Speed: 0.6313 sec/batch\n",
      "Epoch 9/10 Batch: 17000 Training loss: 5.5590 Speed: 0.2073 sec/batch\n",
      "Epoch 9/10 Batch: 18000 Training loss: 5.5394 Speed: 0.6324 sec/batch\n",
      "Epoch 10/10 Batch: 19000 Training loss: 5.7862 Speed: 0.1546 sec/batch\n",
      "Epoch 10/10 Batch: 20000 Training loss: 5.5163 Speed: 0.6339 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "window_size = 10\n",
    "batch_size = 1024\n",
    "num_sampled = 100\n",
    "embedding_size = 200\n",
    "vocab_size = len(vocab_to_int)\n",
    "save_dir = 'checkpoints/embed/train'\n",
    "print_every = 1000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "embed_train_graph = tf.Graph()\n",
    "\n",
    "train_embed(embed_train_graph,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            window_size,\n",
    "            subsampled_text,\n",
    "            num_sampled,\n",
    "            embedding_size,\n",
    "            vocab_size,\n",
    "            save_dir,\n",
    "            print_every\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
